#!/bin/bash

# ðŸ¤– CVLetterAI - Setup AutomÃ¡tico de AI Local
# Este script configura Ollama automaticamente

echo "ðŸš€ CVLetterAI - Configurando AI Local com Ollama"
echo "================================================"

# Detectar sistema operacional
if [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS="linux"
elif [[ "$OSTYPE" == "darwin"* ]]; then
    OS="macos"
elif [[ "$OSTYPE" == "msys" || "$OSTYPE" == "win32" ]]; then
    OS="windows"
else
    echo "âŒ Sistema operacional nÃ£o suportado: $OSTYPE"
    exit 1
fi

echo "ðŸ“± Sistema detectado: $OS"

# FunÃ§Ã£o para verificar se Ollama estÃ¡ instalado
check_ollama() {
    if command -v ollama &> /dev/null; then
        echo "âœ… Ollama jÃ¡ estÃ¡ instalado"
        return 0
    else
        echo "âŒ Ollama nÃ£o encontrado"
        return 1
    fi
}

# Instalar Ollama
install_ollama() {
    echo "ðŸ“¦ Instalando Ollama..."
    
    if [[ "$OS" == "windows" ]]; then
        echo "ðŸªŸ Para Windows:"
        echo "1. Acesse: https://ollama.ai"
        echo "2. Baixe o instalador para Windows"
        echo "3. Execute o instalador"
        echo "4. Execute este script novamente"
        exit 1
    else
        curl -fsSL https://ollama.ai/install.sh | sh
        if [ $? -eq 0 ]; then
            echo "âœ… Ollama instalado com sucesso!"
        else
            echo "âŒ Erro ao instalar Ollama"
            exit 1
        fi
    fi
}

# Baixar modelo recomendado
download_model() {
    echo "ðŸ“¥ Baixando modelo recomendado (llama3.2:3b - ~2GB)..."
    
    ollama pull llama3.2:3b
    
    if [ $? -eq 0 ]; then
        echo "âœ… Modelo baixado com sucesso!"
    else
        echo "âŒ Erro ao baixar modelo"
        echo "ðŸ’¡ Tente manualmente: ollama pull llama3.2:3b"
    fi
}

# Iniciar Ollama em background
start_ollama() {
    echo "ðŸ”„ Iniciando servidor Ollama..."
    
    # Verificar se jÃ¡ estÃ¡ rodando
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama jÃ¡ estÃ¡ rodando!"
        return 0
    fi
    
    # Iniciar em background
    nohup ollama serve > /dev/null 2>&1 &
    
    # Aguardar inicializaÃ§Ã£o
    echo "â³ Aguardando inicializaÃ§Ã£o..."
    for i in {1..10}; do
        if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "âœ… Ollama iniciado com sucesso!"
            return 0
        fi
        echo "   Tentativa $i/10..."
        sleep 2
    done
    
    echo "âŒ Timeout ao iniciar Ollama"
    return 1
}

# Testar AI
test_ai() {
    echo "ðŸ§ª Testando AI..."
    
    response=$(ollama run llama3.2:3b "Say hello" 2>/dev/null | head -1)
    
    if [[ -n "$response" ]]; then
        echo "âœ… AI funcionando! Resposta: $response"
    else
        echo "âŒ Erro ao testar AI"
    fi
}

# Configurar .env
setup_env() {
    echo "âš™ï¸  Configurando .env..."
    
    ENV_FILE=".env.local"
    
    # Criar backup se existir
    if [[ -f "$ENV_FILE" ]]; then
        cp "$ENV_FILE" "${ENV_FILE}.backup.$(date +%s)"
        echo "ðŸ“„ Backup criado: ${ENV_FILE}.backup.$(date +%s)"
    fi
    
    # Adicionar configuraÃ§Ãµes
    cat >> "$ENV_FILE" << EOF

# ðŸ¤– AI Local - Ollama (Configurado automaticamente)
AI_PROVIDER=ollama
OPENAI_BASE_URL=http://localhost:11434
AI_MODEL=llama3.2:3b
OLLAMA_AUTO_START=true

EOF

    echo "âœ… ConfiguraÃ§Ã£o adicionada em $ENV_FILE"
}

# FunÃ§Ã£o principal
main() {
    echo ""
    
    # Verificar se Ollama estÃ¡ instalado
    if ! check_ollama; then
        echo "ðŸ“¦ Instalando Ollama..."
        install_ollama
    fi
    
    echo ""
    
    # Baixar modelo
    echo "ðŸ“¥ Verificando modelo..."
    if ! ollama list | grep -q "llama3.2:3b"; then
        download_model
    else
        echo "âœ… Modelo llama3.2:3b jÃ¡ existe"
    fi
    
    echo ""
    
    # Iniciar servidor
    start_ollama
    
    echo ""
    
    # Testar
    test_ai
    
    echo ""
    
    # Configurar .env
    setup_env
    
    echo ""
    echo "ðŸŽ‰ ConfiguraÃ§Ã£o concluÃ­da!"
    echo "==============================================="
    echo "âœ… Ollama instalado e rodando"
    echo "âœ… Modelo llama3.2:3b baixado"
    echo "âœ… Servidor rodando em http://localhost:11434"
    echo "âœ… CVLetterAI configurado"
    echo ""
    echo "ðŸš€ PrÃ³ximos passos:"
    echo "1. npm run dev (se nÃ£o estiver rodando)"
    echo "2. Acesse http://localhost:3000/chat"
    echo "3. Teste o chat com AI local!"
    echo ""
    echo "ðŸ’¡ Comandos Ãºteis:"
    echo "   ollama list           # Ver modelos instalados"
    echo "   ollama pull <model>   # Baixar modelo"
    echo "   ollama run <model>    # Testar modelo"
    echo "   ollama serve          # Iniciar servidor"
}

# Executar
main