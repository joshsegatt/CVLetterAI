# ğŸ§  Guia: Modelo GGUF Local + Web Search HÃ­brido

## ğŸ¯ **O que vocÃª terÃ¡:**
- âœ… **Modelo local GGUF** rodando offline (privacidade total)
- âœ… **Web search automÃ¡tico** quando precisar de dados atuais
- âœ… **Sistema hÃ­brido inteligente** que decide quando usar cada um
- âœ… **Sem dependÃªncia de APIs** externas (opcional)

---

## ğŸ“¥ **1. Instalar Ollama (Gerenciador de Modelos GGUF)**

### Windows:
```powershell
# Baixar e instalar Ollama
winget install Ollama.Ollama
# ou baixe de: https://ollama.com/download
```

### Verificar instalaÃ§Ã£o:
```powershell
ollama --version
```

---

## ğŸ¤– **2. Baixar Modelo GGUF Recomendado**

### Modelos recomendados (em ordem de qualidade):

#### **OpÃ§Ã£o 1: Llama 3.2 (Melhor custo-benefÃ­cio)**
```powershell
ollama pull llama3.2:latest
# Tamanho: ~2GB
# Qualidade: Excelente para chat
# RAM necessÃ¡ria: 4GB
```

#### **OpÃ§Ã£o 2: Llama 3.1 8B (Mais inteligente)**
```powershell
ollama pull llama3.1:8b
# Tamanho: ~4.7GB  
# Qualidade: Superior
# RAM necessÃ¡ria: 6GB
```

#### **OpÃ§Ã£o 3: Qwen 2.5 (RÃ¡pido e eficiente)**
```powershell
ollama pull qwen2.5:7b
# Tamanho: ~4.1GB
# Qualidade: Muito boa
# RAM necessÃ¡ria: 5GB
```

#### **OpÃ§Ã£o 4: Gemma 2 (Google, compacto)**
```powershell
ollama pull gemma2:2b
# Tamanho: ~1.6GB
# Qualidade: Boa para mÃ¡quinas fracas
# RAM necessÃ¡ria: 3GB
```

---

## âš™ï¸ **3. Configurar o Sistema HÃ­brido**

O sistema jÃ¡ estÃ¡ configurado para:

### **Funcionamento AutomÃ¡tico:**
```typescript
// 1. UsuÃ¡rio pergunta sobre CV
"Como fazer um CV para Ã¡rea de TI?"
â†’ Usa modelo LOCAL (offline, privado)

// 2. UsuÃ¡rio pergunta sobre salÃ¡rios atuais
"Qual salÃ¡rio de desenvolvedor em 2025?"
â†’ Ativa WEB SEARCH + modelo local (hÃ­brido)

// 3. UsuÃ¡rio pergunta sobre empresa especÃ­fica
"Como Ã© trabalhar na Microsoft?"
â†’ Ativa WEB SEARCH para dados atuais
```

### **Triggers automÃ¡ticos para Web Search:**
- Palavras como: "atual", "2025", "recente", "salÃ¡rio", "mercado"
- Perguntas sobre empresas especÃ­ficas
- InformaÃ§Ãµes que mudam com o tempo
- Dados de localizaÃ§Ã£o especÃ­fica

---

## ğŸš€ **4. Testar o Sistema**

### **Iniciar Ollama:**
```powershell
# Ollama jÃ¡ inicia automaticamente, mas se precisar:
ollama serve
```

### **Testar modelo local:**
```powershell
ollama run llama3.2:latest
# Digite: "OlÃ¡, vocÃª estÃ¡ funcionando?"
# Ctrl+D para sair
```

### **Testar no CVLetterAI:**
1. Acesse: `http://localhost:3000/chat`
2. Teste offline: "Como estruturar um CV?"
3. Teste hÃ­brido: "SalÃ¡rios de dev em 2025"

---

## ğŸ“Š **5. Verificar Status do Sistema**

O sistema mostra automaticamente:
- ğŸ§  **Offline**: Modelo local respondeu
- ğŸŒ **HÃ­brido**: Local + pesquisa web
- ğŸ” **Web Search**: SÃ³ pesquisa (se local falhar)

---

## ğŸ›ï¸ **6. ConfiguraÃ§Ãµes AvanÃ§adas**

### **Ajustar desempenho do modelo:**
```powershell
# Para mÃ¡quinas mais potentes
ollama run llama3.1:8b --gpu

# Para mÃ¡quinas mais fracas  
ollama run gemma2:2b --num-thread 4
```

### **VariÃ¡veis de ambiente (opcional):**
```bash
# .env.local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
HYBRID_AI_ENABLED=true
WEB_SEARCH_ENABLED=true
```

---

## ğŸ’¡ **Vantagens do Sistema HÃ­brido:**

### **Privacidade Total:**
- âœ… **Dados sensÃ­veis** ficam no seu computador
- âœ… **Sem envio** para servidores externos
- âœ… **Funciona offline** completamente

### **InteligÃªncia Atual:**
- âœ… **InformaÃ§Ãµes atuais** via web search
- âœ… **Dados de mercado** em tempo real
- âœ… **SalÃ¡rios atualizados** por regiÃ£o

### **Performance:**
- âœ… **Resposta rÃ¡pida** (local Ã© mais rÃ¡pido)
- âœ… **Sem limites** de API
- âœ… **Custo zero** de operaÃ§Ã£o

---

## ğŸ”§ **SoluÃ§Ã£o de Problemas**

### **Ollama nÃ£o inicia:**
```powershell
# Reinstalar
ollama uninstall
winget install Ollama.Ollama
```

### **Modelo nÃ£o baixa:**
```powershell
# Verificar espaÃ§o em disco (precisa de 5-10GB)
# Verificar internet
ollama pull llama3.2:latest --verbose
```

### **Sistema hÃ­brido nÃ£o funciona:**
```powershell
# Testar Ollama diretamente
curl http://localhost:11434/api/tags

# Verificar logs do Next.js
npm run dev
```

---

## ğŸ“ˆ **Modelos por Tipo de MÃ¡quina:**

### **Computador Fraco (4GB RAM):**
```powershell
ollama pull gemma2:2b
# Funciona bem, responde rÃ¡pido
```

### **Computador MÃ©dio (8GB RAM):**
```powershell
ollama pull llama3.2:latest
# Melhor opÃ§Ã£o custo-benefÃ­cio
```

### **Computador Potente (16GB+ RAM):**
```powershell
ollama pull llama3.1:8b
ollama pull qwen2.5:14b
# MÃ¡xima qualidade
```

---

## ğŸ¯ **Resultado Final**

VocÃª terÃ¡ um sistema que:
- ğŸ  **Roda em casa** (sem depender de APIs)
- ğŸ§  **Inteligente** (qualidade prÃ³xima ao GPT-3.5)
- ğŸŒ **Conectado** (busca dados atuais quando precisa)
- ğŸ’° **Gratuito** (zero custo operacional)
- ğŸ”’ **Privado** (dados nÃ£o saem do seu PC)

**Tempo total de configuraÃ§Ã£o: 15-30 minutos** â±ï¸