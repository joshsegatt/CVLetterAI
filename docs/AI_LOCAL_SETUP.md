# ü§ñ Guia de AI Local para CVLetterAI

## üéØ **Op√ß√µes Dispon√≠veis (em ordem de recomenda√ß√£o)**

### **1. Ollama - RECOMENDADO ‚≠ê**

**Vantagens:**
- ‚úÖ Muito f√°cil de instalar
- ‚úÖ Consome menos recursos que LM Studio
- ‚úÖ Interface CLI simples
- ‚úÖ Modelos otimizados
- ‚úÖ Funciona offline 100%

**Instala√ß√£o:**

```bash
# 1. Instalar Ollama
# Windows: Baixe em https://ollama.ai
# Linux/Mac: curl -fsSL https://ollama.ai/install.sh | sh

# 2. Baixar modelo (escolha um)
ollama pull llama3.2:3b          # R√°pido, 2GB
ollama pull llama3.1:8b          # Melhor qualidade, 4.7GB  
ollama pull codellama:7b         # Especializado em c√≥digo
ollama pull mistral:7b           # Alternativa ao Llama

# 3. Iniciar servidor
ollama serve

# 4. Testar (opcional)
ollama run llama3.2:3b "Hello, world!"
```

**Configura√ß√£o no CVLetterAI:**
```env
# .env.local
OPENAI_BASE_URL=http://localhost:11434
AI_PROVIDER=ollama
AI_MODEL=llama3.2:3b
```

---

### **2. Transformers.js - Browser AI üåê**

**Vantagens:**
- ‚úÖ Roda 100% no browser do usu√°rio
- ‚úÖ N√£o precisa de servidor local
- ‚úÖ Totalmente privado
- ‚ùå Modelos menores (menos qualidade)
- ‚ùå Consome RAM do browser

**Instala√ß√£o:**

```bash
npm install @xenova/transformers
```

**Ativa√ß√£o:**
Descomente as linhas no arquivo `src/services/ai/unified.ts`

---

### **3. API Local Customizada üõ†Ô∏è**

**Para quem quer controle total:**

```python
# server.py - Exemplo com FastAPI
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio
import json

app = FastAPI()

@app.post("/chat")
async def chat(request: dict):
    async def generate():
        messages = request["messages"]
        # Sua l√≥gica de AI aqui
        for word in ["Ol√°", "mundo", "do", "CVLetterAI!"]:
            yield f"data: {json.dumps({'content': word + ' '})}\n\n"
            await asyncio.sleep(0.1)
    
    return StreamingResponse(generate(), media_type="text/plain")

@app.get("/health")
async def health():
    return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Configura√ß√£o:**
```env
AI_PROVIDER=local-api
OPENAI_BASE_URL=http://localhost:8000
```

---

## üöÄ **Implementa√ß√£o no CVLetterAI**

### **Arquivos Modificados:**

1. **`src/services/ai/ollama.ts`** - Cliente Ollama
2. **`src/services/ai/unified.ts`** - AI Unificado
3. **`src/services/ai/chat.ts`** - Chat atualizado
4. **`src/services/ai/transformers.ts`** - Transformers.js

### **Como Funciona:**

```typescript
// O sistema automaticamente detecta qual AI est√° dispon√≠vel:
// 1¬∫ - Tenta Ollama (localhost:11434)
// 2¬∫ - Tenta LM Studio (localhost:1234) 
// 3¬∫ - Tenta OpenAI (se API_KEY configurada)
// 4¬∫ - Usa Transformers.js (browser)
// 5¬∫ - Mostra instru√ß√µes de instala√ß√£o
```

---

## üîß **Configura√ß√£o Avan√ßada**

### **Vari√°veis de Ambiente:**

```env
# .env.local

# Ollama (Recomendado)
AI_PROVIDER=ollama
OPENAI_BASE_URL=http://localhost:11434
AI_MODEL=llama3.2:3b

# LM Studio
# AI_PROVIDER=lmstudio
# OPENAI_BASE_URL=http://localhost:1234/v1
# AI_MODEL=local-model

# OpenAI (Pago)
# AI_PROVIDER=openai
# OPENAI_API_KEY=sua_chave_aqui
# AI_MODEL=gpt-4o-mini

# API Local
# AI_PROVIDER=local-api
# OPENAI_BASE_URL=http://localhost:8000
# AI_API_KEY=sua_chave_opcional
```

---

## üìä **Compara√ß√£o de Performance**

| Op√ß√£o | Tamanho | RAM | CPU | Qualidade | Velocidade |
|-------|---------|-----|-----|-----------|------------|
| Ollama llama3.2:3b | 2GB | 4GB | M√©dio | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Ollama llama3.1:8b | 4.7GB | 8GB | Alto | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| LM Studio | 3-7GB | 6-12GB | Alto | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Transformers.js | 100MB | 1-2GB | Baixo | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| OpenAI API | 0MB | 0GB | 0 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéØ **Recomenda√ß√£o Final**

### **Para Desenvolvimento:**
1. **Ollama com llama3.2:3b** - Melhor custo-benef√≠cio
2. Fallback para Transformers.js se n√£o conseguir instalar

### **Para Produ√ß√£o:**
1. **OpenAI API** - M√°xima qualidade e confiabilidade
2. **Ollama em servidor dedicado** - Se privacidade √© cr√≠tica

### **Para Demonstra√ß√£o:**
1. **Transformers.js** - Funciona em qualquer lugar, sem setup

---

## üîß **Pr√≥ximos Passos**

1. Escolha uma op√ß√£o acima
2. Instale seguindo as instru√ß√µes
3. Configure o `.env.local`
4. Reinicie o servidor: `npm run dev`
5. Teste no chat do CVLetterAI

O sistema detectar√° automaticamente qual AI est√° dispon√≠vel! üöÄ